<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <title>Michael Ginn - Home</title>
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- <link href="http://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css"> -->
        <link href="styles/style.css" rel="stylesheet" type="text/css" />
    </head>
    <body>
        <nav class="navbar container">
            <div class="navbar-title">
                <!-- <img src="assets/logo.png" id="logo" /> -->
                <!-- <div id="navbar-title-text">Michael Ginn</div> -->
            </div>
            <div class="navbar-items">
                <a href="index.html" class="navbar-item active">home</a>
                <a href="publications.html" class="navbar-item">publications</a>
                <a href="projects.html" class="navbar-item">projects</a>
                <a href="thoughts.html" class="navbar-item">thoughts</a>
                <a href="assets/cv.pdf" class="navbar-item" target="_blank"
                    >cv</a
                >
            </div>
        </nav>
        <main class="container">
            <h1>michael ginn</h1>
            <p class="subtitle">/maɪkəl dʒɪn/</p>

            <img
                id="main-photo"
                src="assets/IMG_3281.jpeg"
                alt="Headshot of me"
            />
            <p>
                I am a Ph.D. student at the University of Colorado in the
                <a href="lecs-lab.github.io">LECS Lab</a>, supervised by
                <a href="https://alexispalmer.github.io">Prof. Alexis Palmer</a>
                and
                <a href="https://verbs.colorado.edu/~mahu0110/"
                    >Prof. Mans Hulden</a
                >. I am in the Department of Computer Science and the Institute
                for Cognitive Science, where I study
                <b>computational linguistics</b> and
                <b>natural language processing</b>. I obtained my bachelor's in
                Computer Science, with a second major in Linguistics, from
                Washington University in St. Louis in 2022.
            </p>
            <h2>Research Interests</h2>
            <p>
                Broadly, I am interested in exploring the ways modern NLP
                techniques such as large language models (LLMs) can best aid
                underresourced languages. This includes the following:
            </p>

            <ul>
                <li>
                    <b>Morphology and tokenization</b><br />
                    <i
                        >How can we design tokenization methods that are
                        effective with limited data and languages with high
                        morphological complexity? What do token embeddings learn
                        about morphological structure?</i
                    >
                </li>
                <li>
                    <b>LLMs and rare languages</b><br />
                    <i
                        >How can LLMs be adapted to rare languages that are not
                        in their training data? Can we leverage language
                        reference materials such as grammars, dictionaries, and
                        language learning courses to supplement limited data?</i
                    >
                </li>
                <li>
                    <b>Finite-state automata</b><br />
                    <i
                        >Can we effectively interpret neural models with
                        finite-state approximations? Can FSTs be integrated into
                        neurosymbolic systems to improve efficiency and
                        interpretability?</i
                    >
                </li>
                <li>
                    <b>Data augmentation</b><br />
                    <i
                        >What strategies for augmenting existing language data
                        are most effective? Why does augmentation work at
                        all?</i
                    >
                </li>
                <li>
                    <b>In-context learning</b><br />
                    <i
                        >What is the effect of the number, selection strategy,
                        and the presentation method of demonstrations? How does
                        crosslingual ICL differ from English ICL?</i
                    >
                </li>
            </ul>

            <h2>Industry Experience</h2>
            <p>
                I've worked in software engineering as a four-time intern at
                Apple working on localization software, machine learning, and
                large language models. I contributed to the finetuning framework
                for the
                <a href="https://arxiv.org/abs/2403.09611">Apple MM1 model</a>
                used by teams throughout Apple and am currently working on
                predictive code editing. I've worked as a student researcher at
                <a href="https://xmentium.com">xMentium</a> where I explored
                large language models and domain adaptation for legal texts.
                I've built several apps as a an independent
                <a
                    href="https://apps.apple.com/lt/developer/michael-ginn/id1416885467"
                    >iOS and macOS developer</a
                >
                and working as a student software engineer at
                <a href="https://magnifyyourvoice.com">Magnify Your Voice</a>.
            </p>
        </main>
        <footer class="container">
            © Michael Ginn. Contact:
            <i>michael dot ginn at colorado dot edu</i>.
        </footer>
    </body>
</html>

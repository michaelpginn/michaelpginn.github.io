<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <title>Michael Ginn - Home</title>
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- <link href="http://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css"> -->
        <link href="styles/style.css" rel="stylesheet" type="text/css" />
    </head>
    <body>
        <nav class="navbar container">
            <div class="navbar-title">
                <!-- <img src="assets/logo.png" id="logo" /> -->
                <!-- <div id="navbar-title-text">Michael Ginn</div> -->
            </div>
            <div class="navbar-items">
                <a href="index.html" class="navbar-item active">home</a>
                <a href="publications.html" class="navbar-item">publications</a>
                <a href="projects.html" class="navbar-item">projects</a>
                <a href="thoughts.html" class="navbar-item">thoughts</a>
                <a href="assets/cv.pdf" class="navbar-item" target="_blank"
                    >cv</a
                >
            </div>
        </nav>
        <main class="container">
            <h1>Michael Ginn</h1>
            <p class="subtitle">/maɪkəl dʒɪn/</p>

            <img
                id="main-photo"
                src="assets/IMG_3281.jpeg"
                alt="Headshot of me"
            />
            <p>
                I am a fourth-year Ph.D. student at the University of Colorado
                in the
                <a href="lecs-lab.github.io">LECS Lab</a>, supervised by
                <a href="https://alexispalmer.github.io">Prof. Alexis Palmer</a>
                and
                <a href="https://verbs.colorado.edu/~mahu0110/"
                    >Prof. Mans Hulden</a
                >. I'm in the Dept. of Computer Science and the Institute for
                Cognitive Science studying
                <b>natural language processing</b> and
                <b>computational linguistics</b>. I obtained my bachelor's in CS
                at WashU in 2022.
            </p>
            <h2>Research Interests</h2>

            <!--<p>
                Broadly, I am interested in <b>long tail phenomena</b> in large
                language models, training <b>very low-resource ML models</b>,
                neurosymbolic AI, and continual learning.
            </p>-->

            <ul>
                <li>
                    <b>Long-tail phenomena in LLMs</b><br />
                    <i
                        >How effective and predictable are LLMs on long-tail
                        tasks in rare languages, such as translation and
                        glossing? How can we reliably improve performance using
                        labeled examples? How do LLMs model the long-tail, and
                        can we get insights into mechanistic interventions?
                    </i>
                </li>
                <li>
                    <b>Metalearning and Continual Learning</b><br />
                    <i
                        >Can LLMs learn efficiently from descriptions of complex
                        processes and rules, especially metalinguistic
                        information? Under any metalearning approach, can LLMs
                        learn new processes continually, or do they
                        catastrophically forget their original capabilities?</i
                    >
                </li>
                <li>
                    <b>Low-Resource ML and Synthetic Data</b><br />
                    <i
                        >What are the most reliable approaches to train ML
                        models on very small datasets? For tasks with ample
                        domain knowledge or verification (as in low-resource
                        programming languages), can we effectivelly generate
                        synthetic data that provides training benefits?
                    </i>
                </li>
                <li>
                    <b>Neurosymbolic AI</b><br />
                    <i
                        >How can we model dialogue properties using symbolic
                        representations? Can we ever effectively interpret
                        simple neural networks with symbolic machines/formal
                        languages? Can we train LLMs to be robust symbolic
                        reasoners?
                    </i>
                </li>
            </ul>

            <h2>Industry Experience</h2>
            <p>
                Currently, I'm collaborating with Amazon on research modeling
                conversational search dialogs as directed graphs. I've been a
                five-time intern at Apple on various teams: localization
                software, input experience ML, and AI developer tools. Most
                recently, I trained an on-device model using fully synthetic
                data for next-edit prediction in Swift. I also contributed to
                the finetuning framework for the
                <a href="https://arxiv.org/abs/2403.09611">Apple MM1 model</a>
                used by teams throughout Apple.
                <!--I've worked as a student researcher at
                <a href="https://xmentium.com">xMentium</a> where I explored
                large language models and domain adaptation for legal texts.-->
                <!--I've built several apps as a an independent
                <a
                    href="https://apps.apple.com/lt/developer/michael-ginn/id1416885467"
                    >iOS and macOS developer</a
                >
                and working as a student software engineer at
                <a href="https://magnifyyourvoice.com">Magnify Your Voice</a>.-->
            </p>
        </main>
        <footer class="container">
            © Michael Ginn. Contact:
            <i>michael dot ginn at colorado dot edu</i>.
        </footer>
    </body>
</html>

<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <title>Michael Ginn - Projects</title>
        <!-- <link
            href="http://fonts.googleapis.com/css?family=Open+Sans"
            rel="stylesheet"
            type="text/css"
        /> -->
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <link href="styles/style.css" rel="stylesheet" type="text/css" />
        <link href="styles/publications.css" rel="stylesheet" type="text/css" />
    </head>
    <body>
        <nav class="navbar container">
            <div class="navbar-title">
                <!-- <img src="assets/logo.png" id="logo" /> -->
                <!-- <div id="navbar-title-text">LECS Lab</div> -->
            </div>
            <div class="navbar-items">
                <a href="index.html" class="navbar-item">Home</a>
                <a href="publications.html" class="navbar-item">Publications</a>
                <a href="projects.html" class="navbar-item active">Projects</a>
                <a href="assets/cv.pdf" class="navbar-item" target="_blank"
                    >CV</a
                >
            </div>
        </nav>
        <main class="container">
            <h1>Projects</h1>
            <!-- <h2>Preprints</h2> -->
            <!-- ðŸšœ Coming soon... -->
            <h3>SLALLM</h3>
            <p>
                Large language models are highly effective across widely spoken
                languages, but they struggle on languages which are
                underrepresented in their training corpora. In many cases, these
                languages do not have sufficient large-scale corpora, so
                alternate training approaches are necessary.
            </p>

            <p>
                In the Second Language Acquisition for LLMs project (SLALLM), we
                aim to develop a flexible training framework for LLMs that
                utilizes insights from human language acquisition. Specifically,
                we apply <b>language learning materials</b> such as textbooks
                and online courses. We are designing an iterative framework
                where the model is trained on one lesson at a time, applying
                synthetic data generation, LLM interplay, and training
                techniques such as reinforcement learning from human feedback
                (RLHF) and
            </p>
            <h3>Finite-State Extraction</h3>
            <p>ðŸšœ Coming soon...</p>
            <h3>Automated Language Documentation</h3>
            <p>ðŸšœ Coming soon...</p>
            <h3>Building Endangered Language Technology</h3>
            <p>ðŸšœ Coming soon...</p>
            <!-- The following section is auto-generated by the `create-citations.py` script. -->

            <!-- End of generated content -->
        </main>
        <footer class="container">
            Â© Michael Ginn 2024. Contact:
            <i>michael dot ginn at colorado dot edu</i>.
        </footer>
    </body>
</html>
